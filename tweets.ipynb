{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162e1978",
   "metadata": {},
   "source": [
    "# 1. IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89e3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "# for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88774f69",
   "metadata": {},
   "source": [
    "# 2. IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd47a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='label'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_tweets = pd.read_csv(\"data/processedNegative.csv\")\n",
    "neutral_tweets = pd.read_csv(\"data/processedNeutral.csv\")\n",
    "positive_tweets = pd.read_csv(\"data/processedPositive.csv\")\n",
    "\n",
    "negative_df = pd.DataFrame({'tweets':negative_tweets.columns, 'label':0})\n",
    "neutral_df = pd.DataFrame({'tweets':neutral_tweets.columns, 'label':1})\n",
    "positive_df = pd.DataFrame({'tweets':positive_tweets.columns, 'label':2})\n",
    "\n",
    "data = pd.concat([negative_df, neutral_df, positive_df])\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x = data['label'].value_counts()\n",
    "sns.barplot(x=x.index, y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01184d",
   "metadata": {},
   "source": [
    "# 3. PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd4883e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seem i want to drop this account to wuhyungwon...</td>\n",
       "      <td>seem want drop account wuhyungwon unhappy</td>\n",
       "      <td>[seem, want, drop, account, wuhyungwon, unhappy]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nice gif happy</td>\n",
       "      <td>nice gif happy</td>\n",
       "      <td>[nice, gif, happy]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that would be a great trick happy</td>\n",
       "      <td>would great trick happy</td>\n",
       "      <td>[would, great, trick, happy]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uk parliament votes by to in favour of early n...</td>\n",
       "      <td>uk parliament votes favour early national</td>\n",
       "      <td>[uk, parliament, votes, favour, early, national]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is by far my worst day of since january t...</td>\n",
       "      <td>far worst day since january th unhappy crying ...</td>\n",
       "      <td>[far, worst, day, since, january, th, unhappy,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        preprocessed  \\\n",
       "0  seem i want to drop this account to wuhyungwon...   \n",
       "1                                     nice gif happy   \n",
       "2                  that would be a great trick happy   \n",
       "3  uk parliament votes by to in favour of early n...   \n",
       "4  this is by far my worst day of since january t...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0          seem want drop account wuhyungwon unhappy   \n",
       "1                                     nice gif happy   \n",
       "2                            would great trick happy   \n",
       "3          uk parliament votes favour early national   \n",
       "4  far worst day since january th unhappy crying ...   \n",
       "\n",
       "                                           tokenized  label  \n",
       "0   [seem, want, drop, account, wuhyungwon, unhappy]      0  \n",
       "1                                 [nice, gif, happy]      2  \n",
       "2                       [would, great, trick, happy]      2  \n",
       "3   [uk, parliament, votes, favour, early, national]      1  \n",
       "4  [far, worst, day, since, january, th, unhappy,...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text = text.strip()  \n",
    "    text = re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "data['preprocessed'] = data['tweets'].apply(lambda x: preprocess(x))\n",
    "data['no_stopwords'] = data['preprocessed'].apply(lambda x: stopword(x))\n",
    "data['tokenized'] = data['no_stopwords'].apply(tokenizer.tokenize)\n",
    "data[['preprocessed', 'no_stopwords', 'tokenized', 'label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc215774",
   "metadata": {},
   "source": [
    "# 4. Misspelling correction (choose one)\n",
    "\n",
    "### 4.1. choose this if u want to make correction (take time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154da250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[seem, want, drop, account, wuhyungwon, unhappy]</td>\n",
       "      <td>[seem, want, drop, account, wuhyungwon, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nice, gif, happy]</td>\n",
       "      <td>[nice, if, happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[would, great, trick, happy]</td>\n",
       "      <td>[would, great, trick, happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[uk, parliament, votes, favour, early, national]</td>\n",
       "      <td>[up, parliament, votes, favour, early, national]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[far, worst, day, since, january, th, unhappy,...</td>\n",
       "      <td>[far, worst, day, since, january, th, unhappy,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenized  \\\n",
       "0   [seem, want, drop, account, wuhyungwon, unhappy]   \n",
       "1                                 [nice, gif, happy]   \n",
       "2                       [would, great, trick, happy]   \n",
       "3   [uk, parliament, votes, favour, early, national]   \n",
       "4  [far, worst, day, since, january, th, unhappy,...   \n",
       "\n",
       "                                           corrected  \n",
       "0   [seem, want, drop, account, wuhyungwon, unhappy]  \n",
       "1                                  [nice, if, happy]  \n",
       "2                       [would, great, trick, happy]  \n",
       "3   [up, parliament, votes, favour, early, national]  \n",
       "4  [far, worst, day, since, january, th, unhappy,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spell_correction(list_tokens):\n",
    "    ret_list = []\n",
    "    for token in list_tokens:\n",
    "        w = Word(token)\n",
    "        if (w.spellcheck()[0][1] != 1.0):\n",
    "            ret_list.append(w.correct())\n",
    "        else:\n",
    "            ret_list.append(w)\n",
    "    return ret_list\n",
    "\n",
    "data['corrected'] = data['tokenized'].apply(spell_correction)\n",
    "data[['tokenized', 'corrected']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c73d1e",
   "metadata": {},
   "source": [
    "### 4.2. or choose this if u dont want to make correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ae03b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['corrected'] = data['tokenized']\n",
    "data[['tokenized', 'corrected']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cb32f",
   "metadata": {},
   "source": [
    "# 5. Stemmed/lemmed/stemmed+corr/lemmed+corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7e14e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmed</th>\n",
       "      <th>stemmed+corrected</th>\n",
       "      <th>lemmed+corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[seem, want, drop, account, wuhyungwon, unhappy]</td>\n",
       "      <td>seem want drop account wuhyungwon unhappi</td>\n",
       "      <td>seem want drop account wuhyungwon unhappy</td>\n",
       "      <td>seem want drop account wuhyungwon unhappi</td>\n",
       "      <td>seem want drop account wuhyungwon unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nice, gif, happy]</td>\n",
       "      <td>nice gif happi</td>\n",
       "      <td>nice gif happy</td>\n",
       "      <td>nice if happi</td>\n",
       "      <td>nice if happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[would, great, trick, happy]</td>\n",
       "      <td>would great trick happi</td>\n",
       "      <td>would great trick happy</td>\n",
       "      <td>would great trick happi</td>\n",
       "      <td>would great trick happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[uk, parliament, votes, favour, early, national]</td>\n",
       "      <td>uk parliament vote favour earli nation</td>\n",
       "      <td>uk parliament vote favour early national</td>\n",
       "      <td>up parliament vote favour earli nation</td>\n",
       "      <td>up parliament vote favour early national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[far, worst, day, since, january, th, unhappy,...</td>\n",
       "      <td>far worst day sinc januari th unhappi cri cri</td>\n",
       "      <td>far worst day since january th unhappy cry cry</td>\n",
       "      <td>far worst day sinc januari th unhappi cri cri</td>\n",
       "      <td>far worst day since january th unhappy cry cry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenized  \\\n",
       "0   [seem, want, drop, account, wuhyungwon, unhappy]   \n",
       "1                                 [nice, gif, happy]   \n",
       "2                       [would, great, trick, happy]   \n",
       "3   [uk, parliament, votes, favour, early, national]   \n",
       "4  [far, worst, day, since, january, th, unhappy,...   \n",
       "\n",
       "                                         stemmed  \\\n",
       "0      seem want drop account wuhyungwon unhappi   \n",
       "1                                 nice gif happi   \n",
       "2                        would great trick happi   \n",
       "3         uk parliament vote favour earli nation   \n",
       "4  far worst day sinc januari th unhappi cri cri   \n",
       "\n",
       "                                           lemmed  \\\n",
       "0       seem want drop account wuhyungwon unhappy   \n",
       "1                                  nice gif happy   \n",
       "2                         would great trick happy   \n",
       "3        uk parliament vote favour early national   \n",
       "4  far worst day since january th unhappy cry cry   \n",
       "\n",
       "                               stemmed+corrected  \\\n",
       "0      seem want drop account wuhyungwon unhappi   \n",
       "1                                  nice if happi   \n",
       "2                        would great trick happi   \n",
       "3         up parliament vote favour earli nation   \n",
       "4  far worst day sinc januari th unhappi cri cri   \n",
       "\n",
       "                                 lemmed+corrected  \n",
       "0       seem want drop account wuhyungwon unhappy  \n",
       "1                                   nice if happy  \n",
       "2                         would great trick happy  \n",
       "3        up parliament vote favour early national  \n",
       "4  far worst day since january th unhappy cry cry  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lememr = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def stem_words(tokenized_string):\n",
    "    return (\" \".join(stemmer.stem(token) for token in tokenized_string))\n",
    "\n",
    "def lem_words(tokenized_string):\n",
    "    return (\" \".join(lememr.lemmatize(token) for token in tokenized_string))\n",
    "\n",
    "data['stemmed'] = data['tokenized'].apply(lambda x: stem_words(x))\n",
    "data['lemmed'] = data['tokenized'].apply(lambda x: lem_words(x))\n",
    "data['stemmed+corrected'] = data['corrected'].apply(lambda x: stem_words(x))\n",
    "data['lemmed+corrected'] = data['corrected'].apply(lambda x: lem_words(x))\n",
    "data[['tokenized', 'stemmed', 'lemmed', 'stemmed+corrected', 'lemmed+corrected']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a2ba2",
   "metadata": {},
   "source": [
    "# 6. Find similar tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46586173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar(index_list, dataframe):\n",
    "    for i in range(10):\n",
    "        tmp_dict = index_list[i]\n",
    "        for key in tmp_dict.keys():\n",
    "            tmp_key = key\n",
    "        print(\"{0}: {1}\\n{2}: {3}\\n cos:{4}\\n\\n\".format(tmp_key, \n",
    "                                                        dataframe[tmp_key],\n",
    "                                                        tmp_dict[tmp_key][0],\n",
    "                                                        dataframe[tmp_dict[tmp_key][0]],\n",
    "                                                        tmp_dict[tmp_key][1]))\n",
    "\n",
    "def find_similar(dataframe):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(dataframe).toarray()\n",
    "    cos_sim = cosine_similarity(X)\n",
    "    similarity_df = pd.DataFrame(cos_sim)\n",
    "\n",
    "    similarity_list = []\n",
    "\n",
    "    for i in range(similarity_df.shape[0]):\n",
    "        x = similarity_df[i].nlargest(n=2)\n",
    "        if x.iloc[1] >= 0.95 and i != x.index[1]:\n",
    "            similarity_list.append({i:[x.index[1], x.iloc[1]]})\n",
    "\n",
    "    print_most_similar(similarity_list, dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8bf27",
   "metadata": {},
   "source": [
    "## 6.1. Similar default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08b71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: That would be a great trick happy\n",
      "832: That would be a great trick happy.6\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "10: Good evening happy.1\n",
      "100: Good evening happy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "11: US gives a scare\n",
      "1527: US gives a scare.1\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "13: thanks b happy\n",
      "552: .Thanks happy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "15: YG should have sent them to MCD. I want to see them holding the trophy unhappy  anyways .9\n",
      "909: YG should have sent them to MCD. I want to see them holding the trophy unhappy  anyways \n",
      " cos:1.0\n",
      "\n",
      "\n",
      "18: Definitely my arms unhappy  .4\n",
      "864: Definitely my arms unhappy  .5\n",
      " cos:1.0\n",
      "\n",
      "\n",
      "21: This never happened with our happy.1\n",
      "3464: This never happened with our happy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "36: Thanks for the recent follow Happy to connect happy  have a great Thursday. Get it.2\n",
      "2174: Thanks for the recent follow Happy to connect happy  have a great Thursday. Get it.1\n",
      " cos:1.0\n",
      "\n",
      "\n",
      "46: Stats for the week have arrived. 1 new follower and NO unfollowers happy  via.2\n",
      "785: Stats for the week have arrived. 1 new follower and NO unfollowers happy  via.1\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "64: Thanks for the recent follow Happy to connect happy  have a great Thursday..28\n",
      "506: Thanks for the recent follow Happy to connect happy  have a great Thursday..9\n",
      " cos:0.966091783079296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_similar(data['tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7918d1",
   "metadata": {},
   "source": [
    "## 6.2. Similar stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be66b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: would great trick happi\n",
      "524: would great trick happi\n",
      " cos:1.0\n",
      "\n",
      "\n",
      "5: wait u unhappi\n",
      "56: wait unhappi\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "10: good even happi\n",
      "100: good even happi\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "11: us give scare\n",
      "1527: us give scare\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "13: thank b happi\n",
      "377: thank happi\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "15: yg sent mcd want see hold trophi unhappi anyway\n",
      "909: yg sent mcd want see hold trophi unhappi anyway\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "16: miss unhappi\n",
      "196: miss unhappi\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "18: definit arm unhappi\n",
      "144: definit arm unhappi\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "21: never happen happi\n",
      "3464: never happen happi\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "36: thank recent follow happi connect happi great thursday get\n",
      "1828: thank recent follow happi connect happi great thursday get\n",
      " cos:1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_similar(data['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ecaa55",
   "metadata": {},
   "source": [
    "## 6.3. Similar lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3616cdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: would great trick happy\n",
      "524: would great trick happy\n",
      " cos:1.0\n",
      "\n",
      "\n",
      "5: waited u unhappy\n",
      "56: waited unhappy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "10: good evening happy\n",
      "100: good evening happy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "11: u give scare\n",
      "1527: u give scare\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "13: thanks b happy\n",
      "552: thanks happy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "15: yg sent mcd want see holding trophy unhappy anyways\n",
      "909: yg sent mcd want see holding trophy unhappy anyways\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "16: miss unhappy\n",
      "196: miss unhappy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "18: definitely arm unhappy\n",
      "144: definitely arm unhappy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "21: never happened happy\n",
      "3464: never happened happy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "36: thanks recent follow happy connect happy great thursday get\n",
      "1828: thanks recent follow happy connect happy great thursday get\n",
      " cos:1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_similar(data['lemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ab006",
   "metadata": {},
   "source": [
    "## 6.4. Similar stemmed+corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f603b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: would great trick happi\n",
      "524: would great trick happi\n",
      " cos:1.0\n",
      "\n",
      "\n",
      "5: wait u unhappi\n",
      "56: wait unhappi\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "10: good even happi\n",
      "100: good even happi\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "11: us give scare\n",
      "1527: us give scare\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "13: thank b happi\n",
      "377: thank happi\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "15: g sent mud want see hold trophi unhappi anyway\n",
      "909: g sent mud want see hold trophi unhappi anyway\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "16: miss unhappi\n",
      "196: miss unhappi\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "18: definit arm unhappi\n",
      "144: definit arm unhappi\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "21: never happen happi\n",
      "3464: never happen happi\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "36: thank recent follow happi connect happi great thursday get\n",
      "1828: thank recent follow happi connect happi great thursday get\n",
      " cos:1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_similar(data['stemmed+corrected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea5d5b",
   "metadata": {},
   "source": [
    "## 6.5. Similar lemmed+corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8221e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: would great trick happy\n",
      "524: would great trick happy\n",
      " cos:1.0\n",
      "\n",
      "\n",
      "5: waited u unhappy\n",
      "56: waited unhappy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "10: good evening happy\n",
      "100: good evening happy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "11: u give scare\n",
      "1527: u give scare\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "13: thanks b happy\n",
      "552: thanks happy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "15: g sent mud want see holding trophy unhappy anyways\n",
      "909: g sent mud want see holding trophy unhappy anyways\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "16: miss unhappy\n",
      "196: miss unhappy\n",
      " cos:0.9999999999999998\n",
      "\n",
      "\n",
      "18: definitely arm unhappy\n",
      "144: definitely arm unhappy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "21: never happened happy\n",
      "3464: never happened happy\n",
      " cos:1.0000000000000002\n",
      "\n",
      "\n",
      "36: thanks recent follow happy connect happy great thursday get\n",
      "1828: thanks recent follow happy connect happy great thursday get\n",
      " cos:1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_similar(data['lemmed+corrected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d34e1c",
   "metadata": {},
   "source": [
    "# 7. Model training\n",
    "## 7.1 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bc3fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"lemmed+corrected\"],data[\"label\"],test_size=0.2)\n",
    "\n",
    "#Word2Vec\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac9147",
   "metadata": {},
   "source": [
    "## 7.2 Create TFIDF and W2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a3316c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "\n",
    "    def transform(self, X):\n",
    "            return np.array([\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                         or [np.zeros(self.dim)], axis=0)\n",
    "                         for words in X])\n",
    "\n",
    "data['clean_data_tokens'] = [nltk.word_tokenize(i) for i in data[\"lemmed+corrected\"]]\n",
    "model = Word2Vec(data['clean_data_tokens'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b55fc7",
   "metadata": {},
   "source": [
    "## 7.3 Training TFIDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1b1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86       205\n",
      "           1       0.85      0.96      0.90       322\n",
      "           2       0.90      0.85      0.87       248\n",
      "\n",
      "    accuracy                           0.88       775\n",
      "   macro avg       0.89      0.87      0.88       775\n",
      "weighted avg       0.88      0.88      0.88       775\n",
      "\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545f2e6",
   "metadata": {},
   "source": [
    "## 7.3 Training W2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7163edeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.65      0.69       205\n",
      "           1       0.66      0.90      0.76       322\n",
      "           2       0.93      0.58      0.71       248\n",
      "\n",
      "    accuracy                           0.73       775\n",
      "   macro avg       0.78      0.71      0.72       775\n",
      "weighted avg       0.77      0.73      0.73       775\n",
      "\n",
      "0.7316129032258064\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc27f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
